#Default params for running indexing using Pignlproc scripts
# input: (uncompressed??) Wikipedia XML dump
INPUT=/home/chris/projects/pignlproc/src/test/resources/enwiki-latest-pages-articles-1000.xml
#INPUT=/home/chris/projects/pignlproc/src/test/resources/enwiki-pages-articles-sample.xml
LANG=en

#Configuration
MAX_SPAN_LENGTH=5000
MIN_COUNT=1
NUM_DOCS=1000
N=100

# output directory in HDFS
OUTPUT_DIR=/tmp/output

#Location of the list of URIs to keep
URI_LIST=/home/chris/projects/pignlproc/src/test/resources/uri_list.txt

# location of the stop word list file in HDFS
STOPLIST_NAME=stopwords.en.list
STOPLIST_PATH=/home/chris/projects/pignlproc
# local path to JAR containing UDFs
PIGNLPROC_JAR=/home/chris/projects/pignlproc/target/pignlproc-0.1.0-SNAPSHOT.jar
# number of reducers
DEFAULT_PARALLEL=15

# compression of output dataset
#COMPRESS_OUTPUT=false
#OUTPUT_COMPRESSION_CODEC=org.apache.hadoop.io.compress.GzipCodec
